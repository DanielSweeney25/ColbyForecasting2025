[
  {
    "objectID": "C00_coding.html",
    "href": "C00_coding.html",
    "title": "Coding",
    "section": "",
    "text": "Coding is the practice of writing instructions for computers to follow; computers aren’t clever by themselves - they need to be told what to do. Most coding is text-based; people writing coding instructions into simple text documents. But some coding is graphical or visual. We shall be using text-based coding. We are going to use a free and open source general programming language called R. R programming language has its roots in statistics and science, but it really can be used for anything.\nIn the early days, coding was pretty barebones - all one needed was a text editor and access to the programming language - no frills there, no pretty images, no buttons to push, just typing. As time passed, volunteers added niceties to the text editor, like visualizing plots of data, buttons to save files, colorized text for the typed code, and other bells and whistles. These editors became know as graphical user interfaces (GUI for short.) GUIs keep getting easier and easier for people to use. We will use the GUI known as RStudio. It’s best to think of GUIs as wrappers around the core programming language; they are really nice and pretty, but they can’t do math. The programming language itself (which does do math!), evolved only as it needed to to fix bugs and make general improvements.",
    "crumbs": [
      "Coding"
    ]
  },
  {
    "objectID": "C00_coding.html#loading-the-necessary-tools",
    "href": "C00_coding.html#loading-the-necessary-tools",
    "title": "Coding",
    "section": "4.1 Loading the necessary tools",
    "text": "4.1 Loading the necessary tools\nFor any coding project you will need to access a select number of tools, often stored on your computer in what is called a package library (it’s just a directory/folder really). When the package is loaded from the library, all of the functionality the author built in to that package is exposed for you to use in your project. We have created a single file that will both install (if needed) and load (if not already loaded) each of these packages. It’s easy to run.\nFirst, make sure that you have loaded the project (File &gt; Open Project) if you haven’t already. Then at the R console pane type the following…\n\nsource(\"setup.R\")\n\nAfter a few moments the command prompt will return to focus. Be sure to run that command at the beginning of every new R session or anytime you are adding new functionality.\nNow we are ready to load some data into your R session.",
    "crumbs": [
      "Coding"
    ]
  },
  {
    "objectID": "C00_coding.html#spatial-data",
    "href": "C00_coding.html#spatial-data",
    "title": "Coding",
    "section": "4.2 Spatial data",
    "text": "4.2 Spatial data\nSpatial data is any data that has been assigned to a location on a planet (or even between planets!); that means environmental data is mapped to locations on oblate spheroids (like Earth). The oblate spheroid shape presents interesting but challenging math to the data scientist. Modern spatial data is designed to make data science easier by handling all of the location information in a discrete and standardized manner. By discrete we mean that we don’t have to sweat the details.\n\n4.2.1 Point data\nMany spatial data sets come as point data - locations (longitude, latitude and maybe altitude/depth and/or time) with one or more measurements (temperature, cloudiness, probability of precipitation, abundance of fish, population density, etc) attached to that point. Here is an example of point data about long-term oceanographic monitoring buoys in the Gulf of Maine (“gom”). We’ll read the buoy data into a variable, buoy. Next we can print the result simply by typing the name (or you could type print(buoys) if you like all the extra typing.)\n\nbuoys = gom_buoys()\nbuoys\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -70.4277 ymin: 42.3233 xmax: -65.9267 ymax: 44.10163\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 4\n  name  longname            id                geometry\n* &lt;chr&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;POINT [°]&gt;\n1 wms   Western Maine Shelf B01    (-70.4277 43.18065)\n2 cms   Central Maine Shelf E01     (-69.3578 43.7148)\n3 pb    Penobscot Bay       F01   (-68.99689 44.05495)\n4 ems   Eastern Maine Shelf I01   (-68.11359 44.10163)\n5 jb    Jordan Basin        M01   (-67.88029 43.49041)\n6 nec   Northeast Channel   N01     (-65.9267 42.3233)\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can get the online documention for functions a couple of ways. You can type ?name_of_function, or or help(name_of_function). Try ?gom_buoys as an example.\nSometimes you need more - like seeing the function itself. You can always try typing the function name without any trailing parentheses.\n\ngom_buoys\n\nfunction (form = c(\"table\", \"sf\")[2]) \n{\n    x = structure(list(name = c(\"wms\", \"cms\", \"pb\", \"ems\", \"jb\", \n        \"nec\"), longname = c(\"Western Maine Shelf\", \"Central Maine Shelf\", \n        \"Penobscot Bay\", \"Eastern Maine Shelf\", \"Jordan Basin\", \n        \"Northeast Channel\"), id = c(\"B01\", \"E01\", \"F01\", \"I01\", \n        \"M01\", \"N01\"), lon = c(-70.4277, -69.3578, -68.99689, \n        -68.11359, -67.88029, -65.9267), lat = c(43.18065, 43.7148, \n        44.05495, 44.10163, 43.49041, 42.3233)), row.names = c(NA, \n        -6L), class = c(\"tbl_df\", \"tbl\", \"data.frame\"))\n    if (tolower(form[1]) == \"sf\") \n        x = sf::st_as_sf(x, coords = c(\"lon\", \"lat\"), crs = 4326)\n    x\n}\n&lt;bytecode: 0x7fa388898240&gt;\n\n\nIf that still doesn’t work, we highly recommend trying Rseek.org which is an R-language specific search engine.\n\n\nSo there are 6 buoys, each with an attached attribute “name”, “longname” and “id”, as well as the spatial location datain the “geometry” column (just longitude and latitude in this case). We can easily plot these using the “name” column as a color key. For more on plotting spatial data, see this wiki page.\n\nplot(buoys['id'], axes = TRUE, pch = 16)\n\n\n\n\n\n\n\n\nWell, that’s pretty, but without a shoreline it lacks context.\n\n\n4.2.2 Linestrings and polygon data\nLinestrings (open shapes) and polygons (closed shape) are much like point data, except that each geometry is linestring or polygon. We have a set of polygons/linestring that represent the coastline.\n\ncoast = read_coastline()\ncoast\n\nSimple feature collection with 14 features and 0 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -74.9 ymin: 38.95218 xmax: -65 ymax: 46.06477\nGeodetic CRS:  WGS 84\n# A tibble: 14 × 1\n                                                                            geom\n                                                           &lt;MULTILINESTRING [°]&gt;\n 1 ((-72.1019 41.01504, -72.15127 41.05146, -72.18389 41.04678, -72.28745 41.02…\n 2 ((-73.68745 45.56143, -73.85293 45.51572, -73.96055 45.44141, -73.92021 45.4…\n 3 ((-73.69531 45.5855, -73.57236 45.69448, -73.72466 45.67183, -73.85771 45.57…\n 4 ((-66.32412 44.25732, -66.27378 44.29229, -66.21035 44.39204, -66.25049 44.3…\n 5 ((-68.69077 44.24873, -68.70303 44.23198, -68.70171 44.18267, -68.66118 44.1…\n 6 ((-66.89707 44.62891, -66.7625 44.68179, -66.75337 44.70981, -66.74541 44.79…\n 7 ((-68.29941 44.45649, -68.34702 44.43037, -68.40947 44.36426, -68.41172 44.2…\n 8 ((-71.39307 41.46675, -71.36533 41.48525, -71.35449 41.54229, -71.36431 41.5…\n 9 ((-74.25049 39.52939, -74.1332 39.68076, -74.10674 39.74644, -74.25317 39.55…\n10 ((-74.18818 40.6146, -74.23589 40.5187, -74.18813 40.52285, -74.13853 40.541…\n11 ((-70.67373 41.44854, -70.7605 41.37358, -70.8292 41.35898, -70.7853 41.3274…\n12 ((-71.34624 41.46938, -71.29092 41.4646, -71.24141 41.49194, -71.23203 41.65…\n13 ((-70.0627 41.32847, -70.08662 41.31758, -70.23306 41.28633, -70.05508 41.24…\n14 ((-74.9 39.14709, -74.89702 39.14546, -74.9 39.1329), (-74.9 38.95218, -74.7…\n\n\nIn this case, each record of geometry is a “MULTILINESTRING”, which is a group of one or more linestrings. Note that no other variables are in this table - it’s just the geometry.\nLet’s plot these geometries, and add the points on top.\n\nplot(coast, col = \"orange\", lwd = 2, axes = TRUE, reset = FALSE,\n     main = \"Buoys in the Gulf of Maine\")\nplot(st_geometry(buoys), pch = 1, cex = 0.5, add = TRUE)\ntext(st_geometry(buoys), labels = buoys$id, cex = 0.7, adj = c(1,-0.1))\n\n\n\n\n\n\n\n\n\n\n4.2.3 Array data (aka raster data)\nOften spatial data comes in grids, like regular arrays of pixels. These are great for all sorts of data like satellite images, bathymetry maps and environmental modeling data. We’ll be working with environmental modeling data which we call “Brickman data”. You can learn more about Brickman data in the wiki. We’ll be glossing over the details here, but there’s lots of detail in the wiki.\nWe’ll read in the database that tracks 82 Brickman data files, and then immediately filter out the rows that define the “PRESENT” scenario (where present means 1982–2013) and monthly climatology models.\n\ndb = brickman_database() |&gt;\n  filter(scenario == \"PRESENT\", interval == \"mon\") # note the double '==', it's comparative\ndb\n\n# A tibble: 8 × 4\n  scenario year    interval var  \n  &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;\n1 PRESENT  PRESENT mon      MLD  \n2 PRESENT  PRESENT mon      Sbtm \n3 PRESENT  PRESENT mon      SSS  \n4 PRESENT  PRESENT mon      SST  \n5 PRESENT  PRESENT mon      Tbtm \n6 PRESENT  PRESENT mon      U    \n7 PRESENT  PRESENT mon      V    \n8 PRESENT  PRESENT mon      Xbtm \n\n\nIf you are wondering about filtering a table, be sure to check out the wiki on tabular data to get started.\nYou might be wondering what that |&gt; is doing. It is called a pipe, and it delivers the output of one function to the next function as the first parameter (aka argument). For example, brickman_database() produces a table, that table is immediately passed into filter() to choose rows that match our criteria.\nNow that we have the database listing just the records we want, we pass it to the read_brickman() function.\n\ncurrent = read_brickman(db)\ncurrent\n\nstars object with 3 dimensions and 9 attributes\nattribute(s):\n                Min.      1st Qu.        Median          Mean      3rd Qu.\nMLD     1.011275e+00  5.583339810  15.967359543  18.910421492 2.809953e+01\nSbtm    2.324167e+01 32.136343956  34.232215881  33.507147254 3.491243e+01\nSSS     1.644333e+01 30.735633373  31.104771614  31.492407921 3.203519e+01\nSST    -7.826599e-01  6.434107542  12.359498501  12.151707840 1.763068e+01\nTbtm   -2.676387e-01  3.595118523   6.110801697   6.122372065 7.521761e+00\nU      -2.121380e-01 -0.010892980  -0.002634738  -0.010139401 7.229637e-04\nV      -1.883337e-01 -0.010722862  -0.002858645  -0.008474233 9.565173e-04\nXbtm    3.275602e-06  0.001458065   0.003088348   0.008360344 7.256525e-03\ndepth   5.000000e+00 60.258880615 145.012619019 923.313763739 1.704049e+03\n               Max.  NA's\nMLD    1.066982e+02 59796\nSbtm   3.515742e+01 59796\nSSS    3.559161e+01 59796\nSST    2.643147e+01 59796\nTbtm   2.460999e+01 59796\nU      7.469980e-02 59796\nV      5.264002e-02 59796\nXbtm   1.899681e-01 59796\ndepth  4.964409e+03 59796\ndimension(s):\n      from  to offset    delta refsys point      values x/y\nx        1 121 -74.93  0.08226 WGS 84 FALSE        NULL [x]\ny        1  89  46.08 -0.08226 WGS 84 FALSE        NULL [y]\nmonth    1  12     NA       NA     NA    NA Jan,...,Dec    \n\n\nThis loads quite a complex set of arrays, but they have spatial information attached in the dimensions section. The x and y dimensions represent longitude and latitude respectively. The 3rd dimension, month, is time based.\nHere we plot all 12 months of sea surface temperature, SST. Note the they all share the same color scale so that they are easy to compare.\n\nplot(current['SST'])\n\n\n\n\n\n\n\n\nJust as we are able to plot linestrings/polygons along side points, we can also plot these with arrays (rasters). To do this for one month (“Apr”) of one variable (“SSS”) we simply need to slice that data out of the current variable.\n\napril_sss = current['SSS'] |&gt;\n  slice(\"month\", \"Apr\")\napril_sss\n\nstars object with 2 dimensions and 1 attribute\nattribute(s):\n         Min. 1st Qu.   Median    Mean  3rd Qu.     Max. NA's\nSSS  16.44333 30.8342 31.10334 31.4641 31.93447 35.59161 4983\ndimension(s):\n  from  to offset    delta refsys point x/y\nx    1 121 -74.93  0.08226 WGS 84 FALSE [x]\ny    1  89  46.08 -0.08226 WGS 84 FALSE [y]\n\n\nThen it’s just plot, plot, plot.\n\nplot(april_sss, axes = TRUE, reset = FALSE)\nplot(st_geometry(coast), add = TRUE, col = \"orange\", lwd = 2)\nplot(st_geometry(buoys), add = TRUE, pch = 16, col = \"purple\")\n\n\n\n\n\n\n\n\nWe can plot ALL twelve months of a variable (“SST”) with the coast and points shown. There is one slight modification to be made since a single call to plot() actually gets invoked 12 times for this data. So where do we add in the buoys and coast? Fortunately, we can create what is called a “hook” function - who knows where the name hook came from? Once the hook function is defined, it will be applied to the each of the 12 subplots.\n\n# a little function that gets called just after each sub-plot\n# it simple adds the coast and buoy\nadd_coast_and_buoys = function(){\n  plot(st_geometry(coast), col = \"orange\", lwd = 2, add = TRUE)\n  plot(st_geometry(buoys), pch = 16, col = \"purple\", add = TRUE)\n}\n\n# here we call the plot, and tell R where to call `add_coast_and_buoys()` after\n# each subplot is made\nplot(current['SST'], hook = add_coast_and_buoys)",
    "crumbs": [
      "Coding"
    ]
  },
  {
    "objectID": "C01_observations.html",
    "href": "C01_observations.html",
    "title": "Observations",
    "section": "",
    "text": "Follow this wiki page on obtaining data from OBIS. Keep in mind that you will probably want a species with sufficient number of records in the northwest Atlantic. Just what constitutes “sufficient” is probably subject to some debate, but a couple of hundred as a minumum will be helpful for learning. One thing that might help is to be on alert species that are only congregate in one area such as right along the shoreline or only appear in a few months of the year. It isn’t that those species are not worthy of study, but they may make the learning process harder.\nYou should feel free to get the data for a couple of different species, if one becomes a headache with our given resources, then you can switch easily to another.",
    "crumbs": [
      "Observations"
    ]
  },
  {
    "objectID": "C01_observations.html#basisofrecord",
    "href": "C01_observations.html#basisofrecord",
    "title": "Observations",
    "section": "5.1 basisOfRecord",
    "text": "5.1 basisOfRecord\nNext we should examine the basisOfRecord variable to get an understanding of how these observations were made.\n\nobs |&gt; count(basisOfRecord)\n\nSimple feature collection with 4 features and 2 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: -74.65 ymin: 38.8 xmax: -65.00391 ymax: 45.1333\nGeodetic CRS:  WGS 84\n# A tibble: 4 × 3\n  basisOfRecord              n                                              geom\n* &lt;chr&gt;                  &lt;int&gt;                                    &lt;GEOMETRY [°]&gt;\n1 HumanObservation        9354 MULTIPOINT ((-65.07 42.68), (-65.067 42.65), (-6…\n2 NomenclaturalChecklist     1                        POINT (-65.80602 44.97985)\n3 Occurrence                 1                          POINT (-65.2852 42.6243)\n4 PreservedSpecimen        170 MULTIPOINT ((-67.05534 45.09908), (-66.35 45.133…\n\n\nIf you are using a different species you may have different values for basisOfRecord. Let’s take a closer look at the complete records for one from each group.\n\nhuman = obs |&gt;\n  filter(basisOfRecord == \"HumanObservation\") |&gt;\n  slice(1) |&gt;\n  browse_obis()\n\nPlease point your browser to the following url: \n\n\nhttps://api.obis.org/v3/occurrence/00040fa1-7acd-4731-bf1e-6dc16e30c7d4\n\npreserved = obs |&gt;\n  filter(basisOfRecord == \"PreservedSpecimen\") |&gt;\n  slice(1) |&gt;\n  browse_obis()\n\nPlease point your browser to the following url: \n\n\nhttps://api.obis.org/v3/occurrence/003abd48-a98a-4c2f-adc2-8f1d6f71dfa1\n\nchecklist = obs |&gt;\n  filter(basisOfRecord == \"NomenclaturalChecklist\") |&gt;\n  slice(1) |&gt;\n  browse_obis()\n\nPlease point your browser to the following url: \n\n\nhttps://api.obis.org/v3/occurrence/1b967631-4d90-44d0-b57e-cf71c554ee5c\n\noccurrence = obs |&gt;\n  filter(basisOfRecord == \"Occurrence\") |&gt;\n  slice(1) |&gt;\n  browse_obis()\n\nPlease point your browser to the following url: \n\n\nhttps://api.obis.org/v3/occurrence/ecc45419-d260-465a-ad7e-6000d76782f0\n\n\nNext let’s think about what our minimum requirements might be in oirder to build a model. To answer that we need to think about our environmental covariates in the Brickman data](https://github.com/BigelowLab/ColbyForecasting2025/wiki/Brickman). That data has dimensions of x (longitude), y (latitude) and month. In order to match obseravtions with that data, our observations must be complete in those three variables. Let’s take a look at a summary of the observations which will indicate the number of elements missing in each variable.\n\nsummary(obs)\n\n      id            basisOfRecord        eventDate               year     \n Length:9526        Length:9526        Min.   :1932-09-15   Min.   :1932  \n Class :character   Class :character   1st Qu.:2003-10-02   1st Qu.:2003  \n Mode  :character   Mode  :character   Median :2009-07-11   Median :2009  \n                                       Mean   :2006-10-02   Mean   :2006  \n                                       3rd Qu.:2016-11-05   3rd Qu.:2016  \n                                       Max.   :2021-10-14   Max.   :2021  \n                                       NA's   :7            NA's   :7     \n    month            eventTime         individualCount             geom     \n Length:9526        Length:9526        Min.   : 1.000   POINT        :9526  \n Class :character   Class :character   1st Qu.: 1.000   epsg:4326    :   0  \n Mode  :character   Mode  :character   Median : 1.000   +proj=long...:   0  \n                                       Mean   : 1.112                       \n                                       3rd Qu.: 1.000                       \n                                       Max.   :25.000                       \n                                       NA's   :318",
    "crumbs": [
      "Observations"
    ]
  },
  {
    "objectID": "C01_observations.html#eventdate",
    "href": "C01_observations.html#eventdate",
    "title": "Observations",
    "section": "5.2 eventDate",
    "text": "5.2 eventDate\nFor Mola mola there are some rows where eventDate is NA. We need to filter those. The filter function looks for a vector of TRUE/FALSE values - one for each row. In our case, we test the eventDate column to see if it is NA, but then we reverse the TRUE/FALSE logical with the preceding ! (pronounded “bang!”). This we retain only the rows where eventDate is notNA`, and then we print the summary again.\n\nobs = obs |&gt;\n  filter(!is.na(eventDate))\nsummary(obs)\n\n      id            basisOfRecord        eventDate               year     \n Length:9519        Length:9519        Min.   :1932-09-15   Min.   :1932  \n Class :character   Class :character   1st Qu.:2003-10-02   1st Qu.:2003  \n Mode  :character   Mode  :character   Median :2009-07-11   Median :2009  \n                                       Mean   :2006-10-02   Mean   :2006  \n                                       3rd Qu.:2016-11-05   3rd Qu.:2016  \n                                       Max.   :2021-10-14   Max.   :2021  \n                                                                          \n    month            eventTime         individualCount             geom     \n Length:9519        Length:9519        Min.   : 1.000   POINT        :9519  \n Class :character   Class :character   1st Qu.: 1.000   epsg:4326    :   0  \n Mode  :character   Mode  :character   Median : 1.000   +proj=long...:   0  \n                                       Mean   : 1.112                       \n                                       3rd Qu.: 1.000                       \n                                       Max.   :25.000                       \n                                       NA's   :315",
    "crumbs": [
      "Observations"
    ]
  },
  {
    "objectID": "C01_observations.html#individualcount",
    "href": "C01_observations.html#individualcount",
    "title": "Observations",
    "section": "5.3 individualCount",
    "text": "5.3 individualCount\nThat’s better, but we still have 315 NA values for individualCount. Let’s look at at least one record of those in detail; filter out one, and browse it.\n\nobs |&gt;\n  filter(is.na(individualCount)) |&gt;\n  slice(1) |&gt;\n  browse_obis()\n\nPlease point your browser to the following url: \n\n\nhttps://api.obis.org/v3/occurrence/003abd48-a98a-4c2f-adc2-8f1d6f71dfa1\n\n\nEeek! It’s a carcas that washed up on shore! We checked a number of others, and they are all carcases. Is that a presence? Is that what we model are modeling? If not then we should filer those out.\n\nobs = obs |&gt;\n  filter(!is.na(individualCount))\nsummary(obs)\n\n      id            basisOfRecord        eventDate               year     \n Length:9204        Length:9204        Min.   :1932-09-15   Min.   :1932  \n Class :character   Class :character   1st Qu.:2003-07-26   1st Qu.:2003  \n Mode  :character   Mode  :character   Median :2009-07-11   Median :2009  \n                                       Mean   :2006-08-17   Mean   :2006  \n                                       3rd Qu.:2016-11-05   3rd Qu.:2016  \n                                       Max.   :2021-10-14   Max.   :2021  \n    month            eventTime         individualCount             geom     \n Length:9204        Length:9204        Min.   : 1.000   POINT        :9204  \n Class :character   Class :character   1st Qu.: 1.000   epsg:4326    :   0  \n Mode  :character   Mode  :character   Median : 1.000   +proj=long...:   0  \n                                       Mean   : 1.112                       \n                                       3rd Qu.: 1.000                       \n                                       Max.   :25.000                       \n\n\nWell now one has to wonder about a single observation of 25 animals. Let’s check that out.\n\nobs |&gt;\n  filter(individualCount == 25) |&gt;\n  browse_obis()\n\nPlease point your browser to the following url: \n\n\nhttps://api.obis.org/v3/occurrence/c907349a-2c52-4a51-a69a-5a338c5d492a\n\n\nOK, that seems legitmate. And it is possible, Mola mola can congregate for feeding, mating and possibly for karaoke parties.",
    "crumbs": [
      "Observations"
    ]
  },
  {
    "objectID": "C01_observations.html#year",
    "href": "C01_observations.html#year",
    "title": "Observations",
    "section": "5.4 year",
    "text": "5.4 year\nWe know that the “current” climate scenario for the Brickman model data define “current” as the 1982-2013 window. It’s just an average, and if you have values from 1970 to the current year, you probably are safe in including them. But do your observations fall into those years? Let’s make a plot of the counts per year, with dashed lines shown the Brickman “current” cliamtology period.\n\nggplot(data = obs,\n       mapping = aes(x = year)) + \n  geom_bar() + \n  geom_vline(xintercept = c(1982, 2013), linetype = \"dashed\") + \n  labs(title = \"Counts per year\")\n\n\n\n\n\n\n\n\nFor this species, it seem like it is only the record from 1932 that might be a stretch, so let’s filter that out by rejecting records before 1970. This time, instead of asking for a sumamry, we’ll print the dimensions (rows, columns) of the table.\n\nobs = obs |&gt;\n  filter(year &gt;= 1970)\ndim(obs)\n\n[1] 9203    8\n\n\nThat’s still a lot of records. Now let’s check out the distribution across the months of the year.",
    "crumbs": [
      "Observations"
    ]
  },
  {
    "objectID": "C01_observations.html#month",
    "href": "C01_observations.html#month",
    "title": "Observations",
    "section": "5.5 month",
    "text": "5.5 month\nWe will be making models and predictions for each month of the for the 4 future projection climates. Species and observers do show some seasonality, but it that seasonality so extreme that it might be impossible to model some months because of sparse data? Let’s make a plot of the counts per month.\n\nggplot(data = obs,\n       mapping = aes(x = month)) + \n  geom_bar() + \n  labs(title = \"Counts per month\")\n\n\n\n\n\n\n\n\nOh, rats! By default ggplot plots in alpha-numeric order, which scrambles our month order. To fix that we have to convert the month in a factor type while specifying the order of the factors, and we’ll use the mutate() function to help us.\n\nobs = obs |&gt;\n  mutate(month = factor(month, levels = month.abb))\n\nggplot(data = obs,\n       mapping = aes(x = month)) + \n  geom_bar() + \n  labs(title = \"Counts per month\")\n\n\n\n\n\n\n\n\nThat’s better! So, it may be the for Mola mola we might not be able to successfully model in the cold winter months. That’s good to keep in mind.",
    "crumbs": [
      "Observations"
    ]
  },
  {
    "objectID": "C01_observations.html#geometry",
    "href": "C01_observations.html#geometry",
    "title": "Observations",
    "section": "5.6 geometry",
    "text": "5.6 geometry\nLast, but certainly not least, we should consider the possibility that some observations might be on shore. It happens! We already know that some records included fish that were washed up on shore. It’s possible someone mis-keyed the longitude or latitude when entering the vaklues into the database. It’s alos possible that some observations fall just outside the areas where the Brickman data has values. To look for these points, we’ll load the Brickman mask (defines land vs water. Well, really it defines data vs no-data), and use that for further filtering.\nWe need to load the Brickman database, and then filter it for the static variable called “mask”.\n\ndb = brickman_database() |&gt;\n  filter(scenario == \"STATIC\", var == \"mask\")\nmask = read_brickman(db, add_depth = FALSE)\nmask\n\nstars object with 2 dimensions and 1 attribute\nattribute(s):\n      Min. 1st Qu. Median Mean 3rd Qu. Max. NA's\nmask     1       1      1    1       1    1 4983\ndimension(s):\n  from  to offset    delta refsys point x/y\nx    1 121 -74.93  0.08226 WGS 84 FALSE [x]\ny    1  89  46.08 -0.08226 WGS 84 FALSE [y]\n\n\nLet’s see what our mask looks like with the observations drizzled on top. Because the mask only has values of 1 (data) or NA (no-data). You’ll note that we only want to plot the locations of the observations, so we strip obs of everyhting except its geometery.\n\nplot(mask, breaks = \"equal\", axes = TRUE, reset = FALSE)\nplot(st_geometry(obs), pch = \".\", add = TRUE)\n\n\n\n\n\n\n\n\nMaybe with proper with squinting we can see some that faal into no-data areas. The sure-fire way to tell is to extract the mask values at the point locations.\n\nhitOrMiss = extract_brickman(mask, obs)\nhitOrMiss\n\n# A tibble: 9,203 × 3\n   point name  value\n   &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;\n 1 p0001 mask      1\n 2 p0002 mask      1\n 3 p0003 mask      1\n 4 p0004 mask      1\n 5 p0005 mask      1\n 6 p0006 mask      1\n 7 p0007 mask      1\n 8 p0008 mask      1\n 9 p0009 mask      1\n10 p0010 mask      1\n# ℹ 9,193 more rows\n\n\nOK, let’s tally the “value” variable.\n\ncount(hitOrMiss, value)\n\n# A tibble: 2 × 2\n  value     n\n  &lt;dbl&gt; &lt;int&gt;\n1     1  9170\n2    NA    33\n\n\nOoooo, 33 records in obs don’t line up with values in the mask (or in any Brickman data). We should filter those out; we’ll do so with a filter(). Note that we a “reaching” into the hitOrMiss table to access the value column when we use this hitOrMiss$value. Let’s figure out how many records we have dropped with all of this filtering.\n\nobs = obs |&gt;\n  filter(!is.na(hitOrMiss$value))\ndim_end = dim(obs)\n\ndropped_records = dim_start[1] - dim_end[1]\ndropped_records\n\n[1] 356\n\n\nSo, we dropped 356 records which is about 3.7% of the raw OBIS data. Is it worth all that to drop just 4% of the data? Yes! Models are like all things computer… if you put garbage in you should expect to get garbage back out.",
    "crumbs": [
      "Observations"
    ]
  },
  {
    "objectID": "C02_background.html",
    "href": "C02_background.html",
    "title": "Background",
    "section": "",
    "text": "Traditional ecological surveys are systematic, for a given species survey data sets tell us where the species is found and where it is absent. Using an observational data (like OBIS) set we only know where the species is found, which leaves us guessing about where they might not be found. This difference is what distinguishes a presence-abscence data set from a presence-only data set, and this difference guides the modeling process.\nWhen we model, we are trying to define the environs where we should expect to find a species as well as the environs we would not expect to find a species. We have in hand the locations of observations, and we can extract the environmental data at those locations. But to characterize the less suitable environments we are going to have to sample what is called “background”. We want these background samples to roughly match the regional preferences of the observations; that is we want to avoid having observations that are mostly over Georges Bank while our background samples are primarily around the Bay of Fundy.",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "C02_background.html#sample-background",
    "href": "C02_background.html#sample-background",
    "title": "Background",
    "section": "2.1 Sample background",
    "text": "2.1 Sample background\nWhen we sample the background, we are creating the input for the model if we request that the observations (presences) are joined with the background.\nNext we sample the background as guided by the density map. We’ll ask for 2x as many presences, but it is just a request. We also request that no background point be further than 30km (30000m) from it’s closest presence point.\n\ngreedy_input = sample_background(obs, mask, \n                              n = 2 * nrow(obs),\n                              class_label = \"background\",\n                              method = c(\"dist_max\", 30000),\n                              return_pres = TRUE)\n\nWarning in sample_background(obs, mask, n = 2 * nrow(obs), class_label = \"background\", : There are fewer available cells for raster 'NA' (2459 presences) than the requested 4918 background points. Only 4818 will be returned.\n\ngreedy_input\n\nSimple feature collection with 7277 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -74.89169 ymin: 38.805 xmax: -65.02004 ymax: 45.21401\nGeodetic CRS:  WGS 84\n# A tibble: 7,277 × 2\n   class                geometry\n * &lt;fct&gt;             &lt;POINT [°]&gt;\n 1 presence    (-72.8074 39.056)\n 2 presence      (-71.343 40.52)\n 3 presence  (-68.7691 41.52448)\n 4 presence       (-67.79 43.32)\n 5 presence (-68.44324 42.61177)\n 6 presence    (-72.4328 40.213)\n 7 presence   (-71.8784 40.3569)\n 8 presence      (-65.78 43.195)\n 9 presence       (-70.5 42.767)\n10 presence   (-72.3024 40.1862)\n# ℹ 7,267 more rows\n\n\nYou may encounter a warning message that says, “There are fewer available cells for raster…”. This is useful information, there simply weren’t a lot of non-NA cells to sample from. Let’s plot this.\n\nplot(greedy_input['class'], \n     axes = TRUE,  \n     pch = \".\", \n     extent = mask, \n     main = \"August greedy class distribution\",\n     reset = FALSE)\nplot(coast, col = \"orange\", add = TRUE)\n\n\n\n\n\n\n\n\nHmmm, let’s tally the class labels.\n\ncount(greedy_input, class)\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: -74.89169 ymin: 38.805 xmax: -65.02004 ymax: 45.21401\nGeodetic CRS:  WGS 84\n# A tibble: 2 × 3\n  class          n                                                      geometry\n* &lt;fct&gt;      &lt;int&gt;                                              &lt;MULTIPOINT [°]&gt;\n1 presence    2459 ((-65.07 42.68), (-65.067 42.65), (-65.05 42.583), (-65.05 4…\n2 background  4818 ((-65.02004 42.25251), (-65.02004 42.74609), (-65.1023 42.66…\n\n\nWell, that’s imbalanced with a different number presences than background points. But, on the bright side, the background points are definitely in the region of observations.",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "C02_background.html#thin-by-cell",
    "href": "C02_background.html#thin-by-cell",
    "title": "Background",
    "section": "3.1 Thin by cell",
    "text": "3.1 Thin by cell\nIn this approach we eliminate (thin) presences so that we have no more than one per covariate array cell.\n\ndim_before = dim(obs)\ncat(\"number of rows before cell thinning:\", dim_before[1], \"\\n\")\n\nnumber of rows before cell thinning: 2459 \n\nthinned_obs = thin_by_cell(obs, mask)\ndim_after = dim(thinned_obs)\ncat(\"number of rows after cell thinning:\", dim_after[1], \"\\n\")\n\nnumber of rows after cell thinning: 1204 \n\n\nSo, that dropped quite a few!",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "C02_background.html#make-a-weighted-sampling-map",
    "href": "C02_background.html#make-a-weighted-sampling-map",
    "title": "Background",
    "section": "3.2 Make a weighted sampling map",
    "text": "3.2 Make a weighted sampling map\nThere is a technique we can use to to make a weighted sampling map. Simply counting the number of original observations per cell will indicate where we are most likely to oberve Mola mola.\n\nsamp_weight = rasterize_point_density(obs, mask)\nplot(samp_weight, axes = TRUE, breaks = \"equal\", col = rev(hcl.colors(10)), reset = FALSE)\nplot(coast, col = \"orange\", lwd = 2, add = TRUE)\n\n\n\n\n\n\n\n\nNow let’s take a look at the background, but this time we’ll try to match the count of presences.\n\nconservative_input = sample_background(thinned_obs, samp_weight, \n                              n = 2 * nrow(obs),\n                              class_label = \"background\",\n                              method = \"bias\",\n                              return_pres = TRUE)\n\nWarning in sample_background(thinned_obs, samp_weight, n = 2 * nrow(obs), : There are fewer available cells for raster 'NA' (1204 presences) than the requested 4918 background points. Only 1204 will be returned.\n\ncount(conservative_input, class)\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: -74.5867 ymin: 38.868 xmax: -65.02004 ymax: 45.1333\nGeodetic CRS:  WGS 84\n# A tibble: 2 × 3\n  class          n                                                      geometry\n* &lt;fct&gt;      &lt;int&gt;                                              &lt;MULTIPOINT [°]&gt;\n1 presence    1204 ((-65.05 42.6), (-65.067 42.633), (-65.067 42.617), (-65.19 …\n2 background  1204 ((-65.1023 42.66383), (-65.02004 42.58157), (-65.1023 42.581…\n\n\nWhoa - that’s many fewer background points.\n\nplot(conservative_input['class'], \n     axes = TRUE,  \n     pch = \".\", \n     extent = mask, \n     main = \"August conservative class distribution\",\n     reset = FALSE)\nplot(coast, col = \"orange\", add = TRUE)\n\n\n\n\n\n\n\n\nIt appears that background points are essentially shadowing the thinned presence points.",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "C02_background.html#a-function-we-can-reuse",
    "href": "C02_background.html#a-function-we-can-reuse",
    "title": "Background",
    "section": "5.1 A function we can reuse",
    "text": "5.1 A function we can reuse\nHere we make a function that needs at least three arguments: the complete set of observations, the mask used for sampling (and possibly thinning) and the month to filter the observations. The pseudo-code might look like this…\nfor a given month\n  filter the obs for that month\n  make the greedy model input by sampling the background\n    save the greedy model input\n  thin the obs\n  make the conservative model input by sampling background\n    save the conservative model input\n  return a list the greedy and conservative model inputs\nPhew! That’s a lot of steps. To manually run those steps 12 times would be tedious, so we roll that into a function that we can reuse 12 times instead.\nThis function will have a name, make_model_input_by_month. It’s a long name, but it makes it obvious what it does. First we start with the documentation.\n\n#' Builds greedy and conservative model input data sets for a given month\n#' \n#' @param mon chr the month abbreviation for the month of interest (\"Jan\" by default)\n#' @param obs table, the complete observation data set\n#' @param raster stars, the object that defines the sampling space, usually a mask\n#' @param species chr, the name of the species prepended to the name of the output files.\n#'   (By default \"Mola mola\" which gets converted to \"Mola_mola\")\n#' @param path the output data path to store this data (be default \"model_input\")\n#' @param min_obs num this sets a threshold below which we wont try to make a model. (Default is 3)\n#' @return a named two element list of greedy and conservative model inputs - they are tables\nmake_model_input_by_month  = function(mon = \"Jan\",\n                                      obs = read_observations(\"Mola mola\"),\n                                      raster = NULL,\n                                      species = \"Mola mola\",\n                                      path = data_path(\"model_input\"),\n                                      min_obs = 3){\n  # the user *must* provide a raster\n  if (is.null(raster)) stop(\"please provide a raster\")\n  # filter the obs\n  obs = obs |&gt;\n    filter(month == mon[1])\n  \n  # check that we have at least some records, if not enough then alert the user\n  # and return NULL\n  if (nrow(obs) &lt; min_obs){\n    warning(\"sorry, this month has too few records: \", mon)\n    return(NULL)\n  }\n  \n  # make sure the output path exists, if not, make it\n  make_path(path)\n  \n  \n  # make the greedy model input by sampling the background\n  greedy_input = sample_background(obs, raster,\n                                   n = 2 * nrow(obs),\n                                   class_label = \"background\",\n                                   method = c(\"dist_max\", 30000),\n                                   return_pres = TRUE)\n  # save the greedy data\n  filename = sprintf(\"%s-%s-greedy_input.gpkg\", \n                     gsub(\" \", \"_\", species),\n                     mon)\n  write_sf(greedy_input, file.path(path, filename))\n  \n  # thin the obs\n  thinned_obs = thin_by_cell(obs, raster)\n  \n  # sampling weight\n  samp_weight = rasterize_point_density(obs, raster)\n  \n  # make the conservative model\n  conservative_input = sample_background(thinned_obs, samp_weight,\n                                   n = 2 * nrow(obs),\n                                   class_label = \"background\",\n                                   method = \"bias\",\n                                   return_pres = TRUE)\n  \n  # save the conservative data\n  filename = sprintf(\"%s-%s-conservative_input.gpkg\", \n                     gsub(\" \", \"_\", species),\n                     mon)\n  write_sf(conservative_input, file.path(path,filename))\n  \n  # make a list\n  r = list(greedy = greedy_input, conservative = conservative_input)\n  \n  # return, but disable automatic printing\n  invisible(r)\n}",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "C03_covariates.html",
    "href": "C03_covariates.html",
    "title": "Covariates",
    "section": "",
    "text": "“In the end that was the choice you made, and it doesn’t matter how hard it was to make it. It matters that you did.”\n\nCassandra Clare\nNow we turn our attention to what we know and guess about the environments. We are using the Brickman data to make habitat suitability maps for select species under two climate scenarios (RCP45 and RCP85) at two different times (2055 and 2075) in the future. Each variable we might use is called covariate or predictor. Our covariates are nicely packaged up and tidy, but the reality is that it often requires a good deal of data wrangling if the data are messy.\nOur step here is to make sure that two or more covariates are not highly correlated if they are, then we would likely want to drop all but one.",
    "crumbs": [
      "Covariates"
    ]
  },
  {
    "objectID": "C03_covariates.html#reading-in-the-covariates",
    "href": "C03_covariates.html#reading-in-the-covariates",
    "title": "Covariates",
    "section": "2.1 Reading in the covariates",
    "text": "2.1 Reading in the covariates\nWe’ll read in the Brickman database, then filter two different subsets to read: “STATIC” covariate bathymetry that apply across all scenarios and times and monthly covariates for the “PRESENT” period. Note that depth is automatically included - that’s an option - see ?read_brickman for more information.\n\ndb = brickman_database()\npresent = read_brickman(filter(db, scenario == \"PRESENT\", interval == \"mon\"))\n\nWe have used August before as our example, let’s continue with August.\n\naug = present |&gt;\n  dplyr::slice(\"month\", \"Aug\")",
    "crumbs": [
      "Covariates"
    ]
  },
  {
    "objectID": "C03_covariates.html#make-a-pairs-plot",
    "href": "C03_covariates.html#make-a-pairs-plot",
    "title": "Covariates",
    "section": "2.2 Make a pairs plot",
    "text": "2.2 Make a pairs plot\nA pairs plot is a plot often used in exploratory data analysis. It makes a grid of mini-plots of a set of variables, and reveals the relationships among the variables pair-by-pair. It’s easy to make.\n\npairs(aug)\n\n\n\n\n\n\n\n\nIn the lower left portion of the plot we see paired scatter plots, at upper right we see the correlation values of the pairs, and long the diagonal we see a histogram of each variable. Some pairs are highly correlated, say over 0.7, and to include both in the modeling might not provide us with greater predictive power. It may feel counterintuitive to remove any variables - more data means more information, right? And more information means more informed models. Consider two measurements, human arm length and inseam. We might use these to predict if a person is tall, but since they are probably strongly collinear/correlated do we really need both?",
    "crumbs": [
      "Covariates"
    ]
  },
  {
    "objectID": "C03_covariates.html#identify-the-most-independent-variables-and-the-most-collinear",
    "href": "C03_covariates.html#identify-the-most-independent-variables-and-the-most-collinear",
    "title": "Covariates",
    "section": "2.3 Identify the most independent variables (and the most collinear)",
    "text": "2.3 Identify the most independent variables (and the most collinear)\nWe have a function that can help use select which variables to remove. filter_collinear() returns a listing of variables it suggests we keep. It attaches to the return value an attribute (like a post-it note stuck on a box) that lists the complementary variables that it suggests we drop. We are choosing a particular method, but you can learn more about using R’s help for ?filter_collinear.\n\nkeep = filter_collinear(aug, method = \"vif_step\")\nkeep\n\n[1] \"MLD\"  \"Sbtm\" \"SSS\"  \"SST\"  \"Tbtm\" \"U\"    \"V\"   \nattr(,\"to_remove\")\n[1] \"Xbtm\"  \"depth\"\n\n\nOf course, we can decide to ignore this advice, and pick which ever ones we want including keeping them all.\nWhatever selection of variables we decide to model with, we will save this listing to a file. That way we can refer to it progammatically. But that comes later.",
    "crumbs": [
      "Covariates"
    ]
  },
  {
    "objectID": "C03_covariates.html#a-closer-look-at-the-model-input-data",
    "href": "C03_covariates.html#a-closer-look-at-the-model-input-data",
    "title": "Covariates",
    "section": "2.4 A closer look at the model input data",
    "text": "2.4 A closer look at the model input data\nBefore we do commit to a selection of variables, let’s turn our attention back to our presence-background points, and look at just those chosen values rather than at values drawn form across the entire domain. Let’s open the file that contains the “greedy” model input for August during the PRESENT climate scenario.\n\nmodel_input = read_model_input(scientificname = \"Mola mola\", \n                               approach = \"greedy\", \n                               mon = \"Aug\")\nmodel_input\n\nSimple feature collection with 7277 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -74.89169 ymin: 38.805 xmax: -65.02004 ymax: 45.21401\nGeodetic CRS:  WGS 84\n# A tibble: 7,277 × 2\n   class                    geom\n   &lt;chr&gt;             &lt;POINT [°]&gt;\n 1 presence    (-72.8074 39.056)\n 2 presence      (-71.343 40.52)\n 3 presence  (-68.7691 41.52448)\n 4 presence       (-67.79 43.32)\n 5 presence (-68.44324 42.61177)\n 6 presence    (-72.4328 40.213)\n 7 presence   (-71.8784 40.3569)\n 8 presence      (-65.78 43.195)\n 9 presence       (-70.5 42.767)\n10 presence   (-72.3024 40.1862)\n# ℹ 7,267 more rows\n\n\nNext we’ll extract data values from our August covariates.\n\nvariables = extract_brickman(aug, model_input, form = \"wide\")\nvariables\n\n# A tibble: 7,277 × 10\n   point   MLD  Sbtm   SSS   SST  Tbtm         U         V     Xbtm depth\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1 p0001  5.17  35.0  31.6  23.3  7.50 -0.00161  -0.00340  0.00133  304. \n 2 p0002  4.25  32.8  30.6  21.6  8.15 -0.00420  -0.00206  0.00166   71.6\n 3 p0003  4.64  34.0  30.7  20.2  7.05  0.00168   0.00148  0.000793 138. \n 4 p0004  5.58  34.6  30.7  18.8  7.55  0.00267  -0.000410 0.000957 234. \n 5 p0005  5.04  34.7  30.7  19.0  7.43 -0.00619  -0.00121  0.00224  205. \n 6 p0006  4.01  32.4  30.6  22.0  8.22 -0.00344  -0.000859 0.00126   62.6\n 7 p0007  4.10  32.9  30.5  21.8  8.34 -0.00565  -0.00226  0.00216   71.3\n 8 p0008  3.82  32.4  30.3  18.2  3.56 -0.00702  -0.00431  0.00293   81.6\n 9 p0009  3.20  32.4  30.6  17.9  5.73  0.000275 -0.00101  0.000372  70.6\n10 p0010  4.02  32.9  30.6  22.0  8.62 -0.000900 -0.00148  0.000614  64.9\n# ℹ 7,267 more rows\n\n\nWe are going to call a plotting function, plot_pres_vs_bg(), that wants some of the data from model_input and some of the data in variables. So, we have to do some data wrangling to combine those; we’ll add class to variables and then drop the point column.\n\nvariables = variables |&gt;\n  mutate(class = model_input$class) |&gt;    # the $ extracts a column \n  select(-point)                          # the - means \"deselect\" or \"drop\"\nvariables\n\n# A tibble: 7,277 × 10\n     MLD  Sbtm   SSS   SST  Tbtm         U         V     Xbtm depth class   \n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   \n 1  5.17  35.0  31.6  23.3  7.50 -0.00161  -0.00340  0.00133  304.  presence\n 2  4.25  32.8  30.6  21.6  8.15 -0.00420  -0.00206  0.00166   71.6 presence\n 3  4.64  34.0  30.7  20.2  7.05  0.00168   0.00148  0.000793 138.  presence\n 4  5.58  34.6  30.7  18.8  7.55  0.00267  -0.000410 0.000957 234.  presence\n 5  5.04  34.7  30.7  19.0  7.43 -0.00619  -0.00121  0.00224  205.  presence\n 6  4.01  32.4  30.6  22.0  8.22 -0.00344  -0.000859 0.00126   62.6 presence\n 7  4.10  32.9  30.5  21.8  8.34 -0.00565  -0.00226  0.00216   71.3 presence\n 8  3.82  32.4  30.3  18.2  3.56 -0.00702  -0.00431  0.00293   81.6 presence\n 9  3.20  32.4  30.6  17.9  5.73  0.000275 -0.00101  0.000372  70.6 presence\n10  4.02  32.9  30.6  22.0  8.62 -0.000900 -0.00148  0.000614  64.9 presence\n# ℹ 7,267 more rows\n\n\nFinally, can make a specialized plot comparing our variables for each class: presence and background.\n\nplot_pres_vs_bg(variables, \"class\")\n\n\n\n\n\n\n\n\nHow does this inform our thinking about reducing the number of variables? For which variables do presence and background values mirror each other? Which have the least overlap? We know that the model works by finding optimal combinations of covariates for the species. If there is never a difference between the conditions for presences and background then how will it find the optimal niche conditions?",
    "crumbs": [
      "Covariates"
    ]
  },
  {
    "objectID": "C03_covariates.html#saving-a-file-to-keep-track-of-modeling-choices",
    "href": "C03_covariates.html#saving-a-file-to-keep-track-of-modeling-choices",
    "title": "Covariates",
    "section": "2.5 Saving a file to keep track of modeling choices",
    "text": "2.5 Saving a file to keep track of modeling choices\nYou may have noticed that we write a lot of things to files (aka, “writing to disk”). It’s a useful practice especially when working with a multi-step process. One particular file, a configuration file, is used frequently in data science to store information about the choices we make as we work through our project. Configuration files generally are simple text files that we can easily get the computer to read and write.\nIn R, a confguration is treated as a named list. Each element of a list is named, but beyond that there aren’t any particular rules about confugurations. You can learn more about configurations in this tutorial.\nLet’s make a confuguration list that holds 4 items: version identifier, species name, sampling approach and the names of the variables to model with.\n\ncfg = list(\n  version = \"g_Aug\",               # g for greedy!\n  scientificname = \"Mola mola\",\n  approach = \"greedy\",\n  mon = \"Aug\",\n  keep_vars =  keep)\n\nWe can access by name three ways using what is called “indexing” : using the [[ indexing brackets, using the $ indexing operator or using the getElement() function.\n\ncfg[['scientificname']]\n\n[1] \"Mola mola\"\n\ncfg[[2]]\n\n[1] \"Mola mola\"\n\ncfg$scientificname\n\n[1] \"Mola mola\"\n\ngetElement(cfg, \"scientificname\")\n\n[1] \"Mola mola\"\n\ngetElement(cfg, 2)\n\n[1] \"Mola mola\"\n\n\nNow we’ll write this list to a file. First let’s set up a pathwy where we might store these configurations, and for that matter, to store our modeling files. We’ll make a new directory, models/g008 and write the configuration there. We’ll use the famous “YAML” format to store the file. See the file functions/configuration.R for documentation on reading and writing.\n\nok = make_path(data_path(\"models\")) # make a directory for models\nwrite_configuration(cfg)            \n\nUse the Files pane to navigate to your personal data directory. Open the g_Aug.yaml file - this is what you configuration looks like in YAML. Fortunately we don’t mess manually with these much.",
    "crumbs": [
      "Covariates"
    ]
  },
  {
    "objectID": "C04_models.html",
    "href": "C04_models.html",
    "title": "Models",
    "section": "",
    "text": "All models are wrong, but some are useful.\n\nGeorge Box\nModeling starts with a collection of observations (presence and background for us!) and ends up with a collection of coeefficients that can be used with one or more formulas to make a predicition for the past, the present or the future. We are using modeling specifically to make habitat suitability maps for select species under two climate scenarios (RCP45 and RCP85) at two different times (2055 and 2075) in the future.\nWe can choose from a number of different models: random forest “rf”, maximum entropy “maxent” or “maxnet”, boosted regression trees “brt”, general linear models “glm”, etc. The point of each is to make a mathematical representation of natural occurrences. It is important to consider what those occurences might be - categorical like labels? likelihoods like probabilities? continuous like measurements? Here are examples of each…\nWe are modeling with known observations (presences) and a sampling of the background, so we are trying to model a likelihood that a species will be encountered (and reported) relative to the environmental conditions. We are looking for a model that can produce relative likelihood of an encounter that results in a report.\nWe’ll be using a random forest model (rf). We were inspired to follow this route by using this tidy models tutorial prepared by our colleague Omi Johnson.",
    "crumbs": [
      "Models"
    ]
  },
  {
    "objectID": "C04_models.html#modifying-the-recipe-with-steps",
    "href": "C04_models.html#modifying-the-recipe-with-steps",
    "title": "Models",
    "section": "5.1 Modifying the recipe with steps",
    "text": "5.1 Modifying the recipe with steps\nSteps are cumulative modifications, and that means the order in which they are added matters. These steps comprise the bulk of pre-processing steps.\nSome modifications are applied row-by-row. For example, rows of the input modeling data that have one or more missing values (NAs) can be problematic and they should be removed.\nOther modifications are to manipulate entire columns. Sometimes the recipes requires subsequent steps before the modeling begins in earnest. For example we know from experience that it is often useful to log scale (base 10) depth when working with biological models. If depth and Xbtm have made it this far, you’ll note that each range over 4 or more orders of magnitude. That’s not a problem by itself, but it can introduce a bias toward larger values whenever the mean is computed. So, we’ll add a step for log scaling these, but only if depth and Xbtm have made it this far (this may vary by species.)\n\nrec = rec |&gt; \n  step_naomit()\nif (\"depth\" %in% cfg$keep_vars){\n  rec = rec |&gt;\n    step_log(depth,  base = 10)\n}\nif (\"Xbtm\" %in% cfg$keep_vars){\n  rec = rec |&gt;\n    step_log(Xbtm,  base = 10)\n}\nrec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 7\n\n\n\n\n\n── Operations \n\n\n• Removing rows with NA values in: &lt;none&gt;\n\n\nNext we state that we want to remove variables that might be highly correlated with other variables. If two variables are highly correlated, they will not provide the modeling system with more information, just redundant information which doesn’t neccessarily help. step_corr() accepts a variety of arguments specifying which variables to test to correlation including some convenience selectors like all_numeric(), all_string() and friends. We want all predictors which happen to all be numeric, so we can use all_predictors() or all_numeric_predictors(). Specificity is better then generality so let’s choose numeric predictors.\n\n\n\n\n\n\nNote\n\n\n\nWe have already tested variables for high collinearlity, but here we can add a slightly different filter, high correlation, for the same issue. Since we have dealt with this already we shouldn’t expect that step will change the preprocessing very much. But it is instructive to see it in action.\n\n\n\nrec = rec |&gt; \n  step_corr(all_numeric_predictors())\nrec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 7\n\n\n\n\n\n── Operations \n\n\n• Removing rows with NA values in: &lt;none&gt;\n\n\n• Correlation filter on: all_numeric_predictors()",
    "crumbs": [
      "Models"
    ]
  },
  {
    "objectID": "C04_models.html#add-the-recipe-to-the-workflow",
    "href": "C04_models.html#add-the-recipe-to-the-workflow",
    "title": "Models",
    "section": "5.2 Add the recipe to the workflow",
    "text": "5.2 Add the recipe to the workflow\n\nwflow = wflow |&gt;\n  add_recipe(rec)\nwflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: None\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_naomit()\n• step_corr()",
    "crumbs": [
      "Models"
    ]
  },
  {
    "objectID": "C04_models.html#create-the-model",
    "href": "C04_models.html#create-the-model",
    "title": "Models",
    "section": "6.1 Create the model",
    "text": "6.1 Create the model\nWe create a random forest model, declare that it should be run in classification mode (not regression mode), and then specify that we want to use the ranger modeling engine (as opposed to, say, the randForest engine). We additionally specify that it should be able to produce probablilites of a class not just the class label. We also request that it saves bits of info so that we can compare the relative importance of the covariates.\n\nmodel = rand_forest() |&gt;\n  set_mode(\"classification\") |&gt;\n  set_engine(\"ranger\", probability = TRUE, importance = \"permutation\") \nmodel\n\nRandom Forest Model Specification (classification)\n\nEngine-Specific Arguments:\n  probability = TRUE\n  importance = permutation\n\nComputational engine: ranger \n\n\nWell, that feels underwhelming. We can pass arguments unique to the engine using the set_args() function, but, for now we’ll accept the defaults.",
    "crumbs": [
      "Models"
    ]
  },
  {
    "objectID": "C04_models.html#add-the-model-to-the-workflow",
    "href": "C04_models.html#add-the-model-to-the-workflow",
    "title": "Models",
    "section": "6.2 Add the model to the workflow",
    "text": "6.2 Add the model to the workflow\nNow we simply add the model to the workflow.\n\nwflow = wflow |&gt;\n  add_model(model)\nwflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_naomit()\n• step_corr()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (classification)\n\nEngine-Specific Arguments:\n  probability = TRUE\n  importance = permutation\n\nComputational engine: ranger",
    "crumbs": [
      "Models"
    ]
  },
  {
    "objectID": "C04_models.html#predict-with-the-training-data",
    "href": "C04_models.html#predict-with-the-training-data",
    "title": "Models",
    "section": "8.1 Predict with the training data",
    "text": "8.1 Predict with the training data\nFirst we shall predict with the same data we trained with. The results of this will not really tell us much about our model as it is very circular to predict using the very data used to build the model. So this next section is more about a first pass at using the tools at your disposal.\n\ntrain_pred = predict_table(fitted_wflow, tr_data, type = \"prob\")\ntrain_pred\n\n# A tibble: 5,453 × 4\n   .pred_presence .pred_background .pred      class     \n            &lt;dbl&gt;            &lt;dbl&gt; &lt;fct&gt;      &lt;fct&gt;     \n 1        0.00639            0.994 background background\n 2        0.0532             0.947 background background\n 3        0.266              0.734 background background\n 4        0.202              0.798 background background\n 5        0.029              0.971 background background\n 6        0.0203             0.980 background background\n 7        0.0281             0.972 background background\n 8        0.0778             0.922 background background\n 9        0.00958            0.990 background background\n10        0.0358             0.964 background background\n# ℹ 5,443 more rows\n\n\nHere the variables prepended with a dot . are computed, while the class variable is our original. There are many metrics we can use to determine how well this model predicts. Let’s start with the simplest thing… we can make a simply tally of .pred and class.\n\ncount(train_pred, .pred, class)\n\n# A tibble: 4 × 3\n  .pred      class          n\n  &lt;fct&gt;      &lt;fct&gt;      &lt;int&gt;\n1 presence   presence    1400\n2 presence   background   342\n3 background presence     440\n4 background background  3271\n\n\nThere false positives and false negatives, but many are correct. Of course, this is predicting with the very data we used to train the model; knowing that this is predicicting on training data with some many misses might not inspire confidence. But let’s explore more.",
    "crumbs": [
      "Models"
    ]
  },
  {
    "objectID": "C04_models.html#assess-the-model",
    "href": "C04_models.html#assess-the-model",
    "title": "Models",
    "section": "8.2 Assess the model",
    "text": "8.2 Assess the model\nHewre we walk through a number of common assessment tools. We want to assess a model to ascertain how closely it models reality (or not!) Using the tools is always easy, interpreting the metrics is not always easy.\n\n8.2.1 Confusion matrix\nThe confusion matrix is the next step beyond a simple tally that we made above.\n\ntrain_confmat = conf_mat(train_pred, class, .pred)\ntrain_confmat\n\n            Truth\nPrediction   presence background\n  presence       1400        342\n  background      440       3271\n\n\nYou’ll see this is the same as the simple tally we made, but it comes with handy plotting functionality (shown below). Note that a perfect model would have the upper left and lower right quadrants fully accounting for all points. The lower left quadrant shows us the number of false-negatives while the upper right quadrant shows the number of false-positives.\n\nautoplot(train_confmat, type = \"heatmap\")\n\n\n\n\n\n\n\n\n\n\n8.2.2 ROC and AUC\nThe area under the curve (AUC) of the receiver-operator curve (ROC) is a common metric. AUC values range form 0-1 with 1 reflecting a model that faithfully predicts correctly. Technically an AUC value of 0.5 represents a random model (yup, the result of a coin flip!), so values greater than 0.5 and less than 1.0 are expected.\nFirst we can plot the ROC.\n\nplot_roc(train_pred, class, .pred_presence)\n\n\n\n\n\n\n\n\nWe can assure you from practical experience that this is an atypical ROC. Typically they are not smooth, but this smoothness is an artifact of our use of training data. If you really only need the AUC, you can use the roc_auc() function directly.\n\nroc_auc(train_pred, class,  .pred_presence)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.942\n\n\n\n\n8.2.3 Accuracy\nAccuracy, much like our simple tally above, tells us what fraction of the predictions are correct. Not that here we explicitly provide the predicted class label (not the probability.)\n\naccuracy(train_pred, class, .pred)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.857\n\n\n\n\n8.2.4 Partial dependence plot\nPartial dependence reflects the relative contrubution of each variable influence over it’s full range of values. The output is a grid grid of plots showing the relative distribution of the variable (bars) as well as the relative influenceof the variable (line).\n\npartial_dependence_plot(fitted_wflow, data = tr_data)",
    "crumbs": [
      "Models"
    ]
  },
  {
    "objectID": "C04_models.html#predict-with-the-testing-data",
    "href": "C04_models.html#predict-with-the-testing-data",
    "title": "Models",
    "section": "8.3 Predict with the testing data",
    "text": "8.3 Predict with the testing data\nFinally, we can repeat these steps with the testing data. This should give use better information than using the training data\n\n8.3.1 Predict\n\ntest_data = testing(split_data)\ntest_pred = predict_table(fitted_wflow, test_data, type = \"prob\")\ntest_pred\n\n# A tibble: 1,819 × 4\n   .pred_presence .pred_background .pred      class   \n            &lt;dbl&gt;            &lt;dbl&gt; &lt;fct&gt;      &lt;fct&gt;   \n 1          0.597           0.403  presence   presence\n 2          0.932           0.0682 presence   presence\n 3          0.742           0.258  presence   presence\n 4          0.528           0.472  presence   presence\n 5          0.508           0.492  presence   presence\n 6          0.195           0.805  background presence\n 7          0.165           0.835  background presence\n 8          0.316           0.684  background presence\n 9          0.691           0.309  presence   presence\n10          0.619           0.381  presence   presence\n# ℹ 1,809 more rows\n\n\n\n\n8.3.2 Confusion matrix\n\ntest_confmat = conf_mat(test_pred, class, .pred)\nautoplot(test_confmat, type = \"heatmap\")\n\n\n\n\n\n\n\n\n\n\n8.3.3 ROC/AUC\n\nplot_roc(test_pred, class, .pred_presence)\n\n\n\n\n\n\n\n\nThis ROC is more typical of what we see in regular practice.\n\n\n8.3.4 Accuracy\n\naccuracy(test_pred, class, .pred)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.686\n\n\n\n\n8.3.5 Partial Dependence\n\npartial_dependence_plot(fitted_wflow, data = test_data)",
    "crumbs": [
      "Models"
    ]
  },
  {
    "objectID": "C05_prediction.html",
    "href": "C05_prediction.html",
    "title": "Prediction",
    "section": "",
    "text": "It’s tough to make predictions, especially about the future.\n\nYogi Berra\nFinally we come to the end product of forecasting: the prediction. This last step is actually fairly simple, given a recipe and model (now bundled in a workflow container), run the same data-prep and predicting steps as we did earlier. One modification is that we now want to predict across the entire domain of our Brickman data set. You may recall that we are able to read these arrays, display them and extract point data from them. But we haven’t used them en mass as a variable yet.",
    "crumbs": [
      "Prediction"
    ]
  },
  {
    "objectID": "C05_prediction.html#nowcast",
    "href": "C05_prediction.html#nowcast",
    "title": "Prediction",
    "section": "4.1 Nowcast",
    "text": "4.1 Nowcast\nFirst make the prediction. The function yields a stars array object that has three attributes: .pred_presence, .pred_background and .pred. The leading dot simply gives us the heads up that these three values are all computed. The first two range from 0-1 which implies a probability. The last, .pred, is the class label we would assign if we accept that any .pred_presence &gt;= 0.5 should be considered suitable habitat where a reported observation might occur.\n\nnowcast = predict_stars(wflow, covars)\nnowcast\n\nstars object with 2 dimensions and 3 attributes\nattribute(s):\n .pred_presence  .pred_background         .pred     \n Min.   :0.000   Min.   :0.000     presence  : 625  \n 1st Qu.:0.033   1st Qu.:0.757     background:5161  \n Median :0.098   Median :0.902     NA's      :4983  \n Mean   :0.183   Mean   :0.817                      \n 3rd Qu.:0.243   3rd Qu.:0.967                      \n Max.   :1.000   Max.   :1.000                      \n NA's   :4983    NA's   :4983                       \ndimension(s):\n  from  to offset    delta refsys point x/y\nx    1 121 -74.93  0.08226 WGS 84 FALSE [x]\ny    1  89  46.08 -0.08226 WGS 84 FALSE [y]\n\n\nNow we can plot what is often called a “habitat suitability index” (hsi) map.\n\ncoast = read_coastline()\nplot(nowcast['.pred_presence'], main = \"Nowcast August\", \n     axes = TRUE, breaks = seq(0, 1, by = 0.1), reset = FALSE)\nplot(coast, col = \"orange\", lwd = 2, add = TRUE)\n\n\n\n\n\n\n\n\nWe can also plot a presence/background labeled map, but keep in mind it is just a thresholded version of the above where “presence” means .pred_presence &gt;= 0.5.\n\nplot(nowcast['.pred'], main = \"Nowcast August Labels\", \n     axes = TRUE, reset = FALSE)\nplot(coast, col = \"black\", lwd = 2, add = TRUE)",
    "crumbs": [
      "Prediction"
    ]
  },
  {
    "objectID": "C05_prediction.html#forecast",
    "href": "C05_prediction.html#forecast",
    "title": "Prediction",
    "section": "4.2 Forecast",
    "text": "4.2 Forecast\nNow let’s try our hand at forecasting - let’s try RCP85 in 2075. First we load those parameters, then run the prediction and plot.\n\ncovars_rcp85_2075 = read_brickman(db |&gt; filter(scenario == \"RCP85\", year == 2075, interval == \"mon\")) |&gt;\n  select(all_of(cfg$keep_vars)) |&gt;\n  slice(\"month\", \"Aug\") \n\n\nforecast_2075 = predict_stars(wflow, covars_rcp85_2075)\nforecast_2075\n\nstars object with 2 dimensions and 3 attributes\nattribute(s):\n .pred_presence  .pred_background         .pred     \n Min.   :0.000   Min.   :0.393     presence  :  31  \n 1st Qu.:0.143   1st Qu.:0.684     background:5755  \n Median :0.264   Median :0.736     NA's      :4983  \n Mean   :0.231   Mean   :0.769                      \n 3rd Qu.:0.316   3rd Qu.:0.857                      \n Max.   :0.607   Max.   :1.000                      \n NA's   :4983    NA's   :4983                       \ndimension(s):\n  from  to offset    delta refsys point x/y\nx    1 121 -74.93  0.08226 WGS 84 FALSE [x]\ny    1  89  46.08 -0.08226 WGS 84 FALSE [y]\n\n\n\ncoast = read_coastline()\nplot(forecast_2075['.pred_presence'], main = \"RCP85 2075 August\", \n     axes = TRUE, breaks = seq(0, 1, by = 0.1), reset = FALSE)\nplot(coast, col = \"orange\", lwd = 2, add = TRUE)\n\n\n\n\n\n\n\n\nHmmm, that’s pretty different than what the nowcast predicts.",
    "crumbs": [
      "Prediction"
    ]
  },
  {
    "objectID": "C05_prediction.html#forecast-2055",
    "href": "C05_prediction.html#forecast-2055",
    "title": "Prediction",
    "section": "5.1 Forecast 2055",
    "text": "5.1 Forecast 2055\n\ncovars_rcp85_2055 = read_brickman(db |&gt; filter(scenario == \"RCP85\", year == 2055, interval == \"mon\")) |&gt;\n  select(all_of(cfg$keep_vars)) |&gt;\n  slice(\"month\", \"Aug\") \nforecast_2055 = predict_stars(wflow, covars_rcp85_2055)\nforecast_2055\n\nstars object with 2 dimensions and 3 attributes\nattribute(s):\n .pred_presence  .pred_background         .pred     \n Min.   :0.000   Min.   :0.457     presence  :  20  \n 1st Qu.:0.134   1st Qu.:0.686     background:5766  \n Median :0.265   Median :0.735     NA's      :4983  \n Mean   :0.232   Mean   :0.768                      \n 3rd Qu.:0.314   3rd Qu.:0.866                      \n Max.   :0.543   Max.   :1.000                      \n NA's   :4983    NA's   :4983                       \ndimension(s):\n  from  to offset    delta refsys point x/y\nx    1 121 -74.93  0.08226 WGS 84 FALSE [x]\ny    1  89  46.08 -0.08226 WGS 84 FALSE [y]",
    "crumbs": [
      "Prediction"
    ]
  },
  {
    "objectID": "C05_prediction.html#bind-time-series",
    "href": "C05_prediction.html#bind-time-series",
    "title": "Prediction",
    "section": "5.2 Bind time series",
    "text": "5.2 Bind time series\nWe want to bind the .pred_presence attribute for each of the predictions (nowcast, forecast_2055 and forecast_2075). Let’s assume the “present” mean 2020 so we can assign a year.\n\nrcp85 = c(nowcast, forecast_2055, forecast_2075, along = list(year = c(\"2020\", \"2055\", \"2075\")))\n\n\n\n\n\n\n\nNote\n\n\n\nCurious about we provide year as a vector of characters instead of a vector of integers? Try running the command above again and check out the 3rd dimension.\n\n\nSince we are plotting multiple arrays, we need to plot the coastline using a “hook” function.\n\nplot_coast = function(){\n  plot(coast, col = \"orange\", lwd = 2, add = TRUE)\n}\n\nplot(rcp85['.pred_presence'], \n     hook = plot_coast,\n     axes = TRUE, breaks = seq(0, 1, by = 0.1), join_zlim  = TRUE, reset = FALSE)\n\n\n\n\n\n\n\n\nHmmmm. Why does there seem to be a strong shift between 2020 and 2055, while the 2055 to 2075 shift seems less pronounced?\n\n\n\n\n\n\nNote\n\n\n\nDon’t forget that there are other ways to plot array based spatial data.",
    "crumbs": [
      "Prediction"
    ]
  },
  {
    "objectID": "C05_prediction.html#save-the-predictions",
    "href": "C05_prediction.html#save-the-predictions",
    "title": "Prediction",
    "section": "5.3 Save the predictions",
    "text": "5.3 Save the predictions\nWe could save all three attributes, but .pred_background is just 1 - .pred_presence, and .pred is just coding “presence” where .pred_presence &gt;= 0.5, so we can always compute those as needed if we have .pred_presence. In that case, let’s just save the first attribute, .pred_presence, in a multilayer GeoTIFF formatted image array file. The write_prediction() function will do just that.\n\n# make sure the output directory exists\npath = data_path(\"predictions\")\nif (!dir.exists(path)) ok = dir.create(path, recursive = TRUE)\n\n# write individual arrays?\nwrite_prediction(nowcast, file = file.path(path,\"g_Aug_RCP85_2020.tif\"))\nwrite_prediction(forecast_2055, file = file.path(path, \"g_Aug_RCP85_2055.tif\"))\nwrite_prediction(forecast_2075, file = file.path(path, \"g_Aug_RCP85_2075.tif\"))\n\n# or write them together in a \"multi-layer\" file?\nwrite_prediction(rcp85, file = file.path(path, \"g_Aug_RCP85_all.tif\"))\n\nTo read it back simply provide the filename to read_prediction(). If you are reading back a multi-layer array, be sure to check out the time argument to assign values to the time dimension. Single layer arrays don’t have the concept of time so the time argument is ignored.",
    "crumbs": [
      "Prediction"
    ]
  },
  {
    "objectID": "S01_observations.html",
    "href": "S01_observations.html",
    "title": "My Observations",
    "section": "",
    "text": "Follow this wiki page on obtaining data from OBIS. Keep in mind that you will probably want a species with sufficient number of records in the northwest Atlantic. Just what constitutes “sufficient” is probably subject to some debate, but a couple of hundred as a minumum will be helpful for learning. One thing that might help is to be on alert species that are only congregate in one area such as right along the shoreline or only appear in a few months of the year. It isn’t that those species are not worthy of study, but they may make the learning process harder.\nYou should feel free to get the data for a couple of different species, if one becomes a headache with our given resources, then you can switch easily to another.",
    "crumbs": [
      "My_observations"
    ]
  },
  {
    "objectID": "S01_observations.html#basisofrecord",
    "href": "S01_observations.html#basisofrecord",
    "title": "My Observations",
    "section": "5.1 basisOfRecord",
    "text": "5.1 basisOfRecord\nNext we should examine the basisOfRecord variable to get an understanding of how these observations were made.\n\nobs |&gt; count(basisOfRecord)\n\nSimple feature collection with 4 features and 2 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: -74.65 ymin: 38.8 xmax: -65.00391 ymax: 45.1333\nGeodetic CRS:  WGS 84\n# A tibble: 4 × 3\n  basisOfRecord              n                                              geom\n* &lt;chr&gt;                  &lt;int&gt;                                    &lt;GEOMETRY [°]&gt;\n1 HumanObservation        9354 MULTIPOINT ((-65.07 42.68), (-65.067 42.65), (-6…\n2 NomenclaturalChecklist     1                        POINT (-65.80602 44.97985)\n3 Occurrence                 1                          POINT (-65.2852 42.6243)\n4 PreservedSpecimen        170 MULTIPOINT ((-67.05534 45.09908), (-66.35 45.133…\n\n\nIf you are using a different species you may have different values for basisOfRecord. Let’s take a closer look at the complete records for one from each group.\n\nhuman = obs |&gt;\n  filter(basisOfRecord == \"HumanObservation\") |&gt;\n  slice(1) |&gt;\n  browse_obis()\n\nPlease point your browser to the following url: \n\n\nhttps://api.obis.org/v3/occurrence/00040fa1-7acd-4731-bf1e-6dc16e30c7d4\n\npreserved = obs |&gt;\n  filter(basisOfRecord == \"PreservedSpecimen\") |&gt;\n  slice(1) |&gt;\n  browse_obis()\n\nPlease point your browser to the following url: \n\n\nhttps://api.obis.org/v3/occurrence/003abd48-a98a-4c2f-adc2-8f1d6f71dfa1\n\nchecklist = obs |&gt;\n  filter(basisOfRecord == \"NomenclaturalChecklist\") |&gt;\n  slice(1) |&gt;\n  browse_obis()\n\nPlease point your browser to the following url: \n\n\nhttps://api.obis.org/v3/occurrence/1b967631-4d90-44d0-b57e-cf71c554ee5c\n\noccurrence = obs |&gt;\n  filter(basisOfRecord == \"Occurrence\") |&gt;\n  slice(1) |&gt;\n  browse_obis()\n\nPlease point your browser to the following url: \n\n\nhttps://api.obis.org/v3/occurrence/ecc45419-d260-465a-ad7e-6000d76782f0\n\n\nNext let’s think about what our minimum requirements might be in oirder to build a model. To answer that we need to think about our environmental covariates in the Brickman data](https://github.com/BigelowLab/ColbyForecasting2025/wiki/Brickman). That data has dimensions of x (longitude), y (latitude) and month. In order to match obseravtions with that data, our observations must be complete in those three variables. Let’s take a look at a summary of the observations which will indicate the number of elements missing in each variable.\n\nsummary(obs)\n\n      id            basisOfRecord        eventDate               year     \n Length:9526        Length:9526        Min.   :1932-09-15   Min.   :1932  \n Class :character   Class :character   1st Qu.:2003-10-02   1st Qu.:2003  \n Mode  :character   Mode  :character   Median :2009-07-11   Median :2009  \n                                       Mean   :2006-10-02   Mean   :2006  \n                                       3rd Qu.:2016-11-05   3rd Qu.:2016  \n                                       Max.   :2021-10-14   Max.   :2021  \n                                       NA's   :7            NA's   :7     \n    month            eventTime         individualCount             geom     \n Length:9526        Length:9526        Min.   : 1.000   POINT        :9526  \n Class :character   Class :character   1st Qu.: 1.000   epsg:4326    :   0  \n Mode  :character   Mode  :character   Median : 1.000   +proj=long...:   0  \n                                       Mean   : 1.112                       \n                                       3rd Qu.: 1.000                       \n                                       Max.   :25.000                       \n                                       NA's   :318",
    "crumbs": [
      "My_observations"
    ]
  },
  {
    "objectID": "S01_observations.html#eventdate",
    "href": "S01_observations.html#eventdate",
    "title": "My Observations",
    "section": "5.2 eventDate",
    "text": "5.2 eventDate\nFor Mola mola there are some rows where eventDate is NA. We need to filter those. The filter function looks for a vector of TRUE/FALSE values - one for each row. In our case, we test the eventDate column to see if it is NA, but then we reverse the TRUE/FALSE logical with the preceding ! (pronounded “bang!”). This we retain only the rows where eventDate is notNA`, and then we print the summary again.\n\nobs = obs |&gt;\n  filter(!is.na(eventDate))\nsummary(obs)\n\n      id            basisOfRecord        eventDate               year     \n Length:9519        Length:9519        Min.   :1932-09-15   Min.   :1932  \n Class :character   Class :character   1st Qu.:2003-10-02   1st Qu.:2003  \n Mode  :character   Mode  :character   Median :2009-07-11   Median :2009  \n                                       Mean   :2006-10-02   Mean   :2006  \n                                       3rd Qu.:2016-11-05   3rd Qu.:2016  \n                                       Max.   :2021-10-14   Max.   :2021  \n                                                                          \n    month            eventTime         individualCount             geom     \n Length:9519        Length:9519        Min.   : 1.000   POINT        :9519  \n Class :character   Class :character   1st Qu.: 1.000   epsg:4326    :   0  \n Mode  :character   Mode  :character   Median : 1.000   +proj=long...:   0  \n                                       Mean   : 1.112                       \n                                       3rd Qu.: 1.000                       \n                                       Max.   :25.000                       \n                                       NA's   :315",
    "crumbs": [
      "My_observations"
    ]
  },
  {
    "objectID": "S01_observations.html#individualcount",
    "href": "S01_observations.html#individualcount",
    "title": "My Observations",
    "section": "5.3 individualCount",
    "text": "5.3 individualCount\nThat’s better, but we still have 315 NA values for individualCount. Let’s look at at least one record of those in detail; filter out one, and browse it.\n\nobs |&gt;\n  filter(is.na(individualCount)) |&gt;\n  slice(1) |&gt;\n  browse_obis()\n\nPlease point your browser to the following url: \n\n\nhttps://api.obis.org/v3/occurrence/003abd48-a98a-4c2f-adc2-8f1d6f71dfa1\n\n\nEeek! It’s a carcas that washed up on shore! We checked a number of others, and they are all carcases. Is that a presence? Is that what we model are modeling? If not then we should filer those out.\n\nobs = obs |&gt;\n  filter(!is.na(individualCount))\nsummary(obs)\n\n      id            basisOfRecord        eventDate               year     \n Length:9204        Length:9204        Min.   :1932-09-15   Min.   :1932  \n Class :character   Class :character   1st Qu.:2003-07-26   1st Qu.:2003  \n Mode  :character   Mode  :character   Median :2009-07-11   Median :2009  \n                                       Mean   :2006-08-17   Mean   :2006  \n                                       3rd Qu.:2016-11-05   3rd Qu.:2016  \n                                       Max.   :2021-10-14   Max.   :2021  \n    month            eventTime         individualCount             geom     \n Length:9204        Length:9204        Min.   : 1.000   POINT        :9204  \n Class :character   Class :character   1st Qu.: 1.000   epsg:4326    :   0  \n Mode  :character   Mode  :character   Median : 1.000   +proj=long...:   0  \n                                       Mean   : 1.112                       \n                                       3rd Qu.: 1.000                       \n                                       Max.   :25.000                       \n\n\nWell now one has to wonder about a single observation of 25 animals. Let’s check that out.\n\nobs |&gt;\n  filter(individualCount == 25) |&gt;\n  browse_obis()\n\nPlease point your browser to the following url: \n\n\nhttps://api.obis.org/v3/occurrence/c907349a-2c52-4a51-a69a-5a338c5d492a\n\n\nOK, that seems legitmate. And it is possible, Mola mola can congregate for feeding, mating and possibly for karaoke parties.",
    "crumbs": [
      "My_observations"
    ]
  },
  {
    "objectID": "S01_observations.html#year",
    "href": "S01_observations.html#year",
    "title": "My Observations",
    "section": "5.4 year",
    "text": "5.4 year\nWe know that the “current” climate scenario for the Brickman model data define “current” as the 1982-2013 window. It’s just an average, and if you have values from 1970 to the current year, you probably are safe in including them. But do your observations fall into those years? Let’s make a plot of the counts per year, with dashed lines shown the Brickman “current” cliamtology period.\n\nggplot(data = obs,\n       mapping = aes(x = year)) + \n  geom_bar() + \n  geom_vline(xintercept = c(1982, 2013), linetype = \"dashed\") + \n  labs(title = \"Counts per year\")\n\n\n\n\n\n\n\n\nFor this species, it seem like it is only the record from 1932 that might be a stretch, so let’s filter that out by rejecting records before 1970. This time, instead of asking for a sumamry, we’ll print the dimensions (rows, columns) of the table.\n\nobs = obs |&gt;\n  filter(year &gt;= 1970)\ndim(obs)\n\n[1] 9203    8\n\n\nThat’s still a lot of records. Now let’s check out the distribution across the months of the year.",
    "crumbs": [
      "My_observations"
    ]
  },
  {
    "objectID": "S01_observations.html#month",
    "href": "S01_observations.html#month",
    "title": "My Observations",
    "section": "5.5 month",
    "text": "5.5 month\nWe will be making models and predictions for each month of the for the 4 future projection climates. Species and observers do show some seasonality, but it that seasonality so extreme that it might be impossible to model some months because of sparse data? Let’s make a plot of the counts per month.\n\nggplot(data = obs,\n       mapping = aes(x = month)) + \n  geom_bar() + \n  labs(title = \"Counts per month\")\n\n\n\n\n\n\n\n\nOh, rats! By default ggplot plots in alpha-numeric order, which scrambles our month order. To fix that we have to convert the month in a factor type while specifying the order of the factors, and we’ll use the mutate() function to help us.\n\nobs = obs |&gt;\n  mutate(month = factor(month, levels = month.abb))\n\nggplot(data = obs,\n       mapping = aes(x = month)) + \n  geom_bar() + \n  labs(title = \"Counts per month\")\n\n\n\n\n\n\n\n\nThat’s better! So, it may be the for Mola mola we might not be able to successfully model in the cold winter months. That’s good to keep in mind.",
    "crumbs": [
      "My_observations"
    ]
  },
  {
    "objectID": "S01_observations.html#geometry",
    "href": "S01_observations.html#geometry",
    "title": "My Observations",
    "section": "5.6 geometry",
    "text": "5.6 geometry\nLast, but certainly not least, we should consider the possibility that some observations might be on shore. It happens! We already know that some records included fish that were washed up on shore. It’s possible someone mis-keyed the longitude or latitude when entering the vaklues into the database. It’s alos possible that some observations fall just outside the areas where the Brickman data has values. To look for these points, we’ll load the Brickman mask (defines land vs water. Well, really it defines data vs no-data), and use that for further filtering.\nWe need to load the Brickman database, and then filter it for the static variable called “mask”.\n\ndb = brickman_database() |&gt;\n  filter(scenario == \"STATIC\", var == \"mask\")\nmask = read_brickman(db, add_depth = FALSE)\nmask\n\nstars object with 2 dimensions and 1 attribute\nattribute(s):\n      Min. 1st Qu. Median Mean 3rd Qu. Max. NA's\nmask     1       1      1    1       1    1 4983\ndimension(s):\n  from  to offset    delta refsys point x/y\nx    1 121 -74.93  0.08226 WGS 84 FALSE [x]\ny    1  89  46.08 -0.08226 WGS 84 FALSE [y]\n\n\nLet’s see what our mask looks like with the observations drizzled on top. Because the mask only has values of 1 (data) or NA (no-data). You’ll note that we only want to plot the locations of the observations, so we strip obs of everyhting except its geometery.\n\nplot(mask, breaks = \"equal\", axes = TRUE, reset = FALSE)\nplot(st_geometry(obs), pch = \".\", add = TRUE)\n\n\n\n\n\n\n\n\nMaybe with proper with squinting we can see some that faal into no-data areas. The sure-fire way to tell is to extract the mask values at the point locations.\n\nhitOrMiss = extract_brickman(mask, obs)\nhitOrMiss\n\n# A tibble: 9,203 × 3\n   point name  value\n   &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;\n 1 p0001 mask      1\n 2 p0002 mask      1\n 3 p0003 mask      1\n 4 p0004 mask      1\n 5 p0005 mask      1\n 6 p0006 mask      1\n 7 p0007 mask      1\n 8 p0008 mask      1\n 9 p0009 mask      1\n10 p0010 mask      1\n# ℹ 9,193 more rows\n\n\nOK, let’s tally the “value” variable.\n\ncount(hitOrMiss, value)\n\n# A tibble: 2 × 2\n  value     n\n  &lt;dbl&gt; &lt;int&gt;\n1     1  9170\n2    NA    33\n\n\nOoooo, 33 records in obs don’t line up with values in the mask (or in any Brickman data). We should filter those out; we’ll do so with a filter(). Note that we a “reaching” into the hitOrMiss table to access the value column when we use this hitOrMiss$value. Let’s figure out how many records we have dropped with all of this filtering.\n\nobs = obs |&gt;\n  filter(!is.na(hitOrMiss$value))\ndim_end = dim(obs)\n\ndropped_records = dim_start[1] - dim_end[1]\ndropped_records\n\n[1] 356\n\n\nSo, we dropped 356 records which is about 3.7% of the raw OBIS data. Is it worth all that to drop just 4% of the data? Yes! Models are like all things computer… if you put garbage in you should expect to get garbage back out.",
    "crumbs": [
      "My_observations"
    ]
  },
  {
    "objectID": "S02_background.html",
    "href": "S02_background.html",
    "title": "Background",
    "section": "",
    "text": "Traditional ecological surveys are systematic, for a given species survey data sets tell us where the species is found and where it is absent. Using an observational data (like OBIS) set we only know where the species is found, which leaves us guessing about where they might not be found. This difference is what distinguishes a presence-abscence data set from a presence-only data set, and this difference guides the modeling process.\nWhen we model, we are trying to define the environs where we should expect to find a species as well as the environs we would not expect to find a species. We have in hand the locations of observations, and we can extract the environmental data at those locations. But to characterize the less suitable environments we are going to have to sample what is called “background”. We want these background samples to roughly match the regional preferences of the observations; that is we want to avoid having observations that are mostly over Georges Bank while our background samples are primarily around the Bay of Fundy.",
    "crumbs": [
      "My_background"
    ]
  },
  {
    "objectID": "S02_background.html#sample-background",
    "href": "S02_background.html#sample-background",
    "title": "Background",
    "section": "2.1 Sample background",
    "text": "2.1 Sample background\nWhen we sample the background, we are creating the input for the model if we request that the observations (presences) are joined with the background.\nNext we sample the background as guided by the density map. We’ll ask for 2x as many presences, but it is just a request. We also request that no background point be further than 30km (30000m) from it’s closest presence point.\n\ngreedy_input = sample_background(obs, mask, \n                              n = 2 * nrow(obs),\n                              class_label = \"background\",\n                              method = c(\"dist_max\", 30000),\n                              return_pres = TRUE)\ngreedy_input\n\nSimple feature collection with 5499 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -74.56263 ymin: 38.8 xmax: -65.01667 ymax: 45.54306\nGeodetic CRS:  WGS 84\n# A tibble: 5,499 × 2\n   class                geometry\n * &lt;fct&gt;             &lt;POINT [°]&gt;\n 1 presence (-72.01883 40.67967)\n 2 presence     (-67.61667 40.6)\n 3 presence    (-66.31667 41.45)\n 4 presence (-67.01667 42.16667)\n 5 presence     (-73.03333 39.5)\n 6 presence    (-66.75 41.46667)\n 7 presence    (-66.46667 44.65)\n 8 presence (-66.11667 42.06667)\n 9 presence    (-65.25 45.08333)\n10 presence (-66.46667 41.93333)\n# ℹ 5,489 more rows\n\n\nYou may encounter a warning message that says, “There are fewer available cells for raster…”. This is useful information, there simply weren’t a lot of non-NA cells to sample from. Let’s plot this.\n\nplot(greedy_input['class'], \n     axes = TRUE,  \n     pch = \".\", \n     extent = mask, \n     main = \"August greedy class distribution\",\n     reset = FALSE)\nplot(coast, col = \"orange\", add = TRUE)\n\n\n\n\n\n\n\n\nHmmm, let’s tally the class labels.\n\ncount(greedy_input, class)\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: -74.56263 ymin: 38.8 xmax: -65.01667 ymax: 45.54306\nGeodetic CRS:  WGS 84\n# A tibble: 2 × 3\n  class          n                                                      geometry\n* &lt;fct&gt;      &lt;int&gt;                                              &lt;MULTIPOINT [°]&gt;\n1 presence    1833 ((-65.71667 42.46667), (-65.63333 42.65), (-65.91667 42.9166…\n2 background  3666 ((-65.43136 42.33478), (-65.34909 42.41704), (-65.34909 42.4…\n\n\nWell, that’s imbalanced with a different number presences than background points. But, on the bright side, the background points are definitely in the region of observations.",
    "crumbs": [
      "My_background"
    ]
  },
  {
    "objectID": "S02_background.html#thin-by-cell",
    "href": "S02_background.html#thin-by-cell",
    "title": "Background",
    "section": "3.1 Thin by cell",
    "text": "3.1 Thin by cell\nIn this approach we eliminate (thin) presences so that we have no more than one per covariate array cell.\n\ndim_before = dim(obs)\ncat(\"number of rows before cell thinning:\", dim_before[1], \"\\n\")\n\nnumber of rows before cell thinning: 1833 \n\nthinned_obs = thin_by_cell(obs, mask)\ndim_after = dim(thinned_obs)\ncat(\"number of rows after cell thinning:\", dim_after[1], \"\\n\")\n\nnumber of rows after cell thinning: 976 \n\n\nSo, that dropped quite a few!",
    "crumbs": [
      "My_background"
    ]
  },
  {
    "objectID": "S02_background.html#make-a-weighted-sampling-map",
    "href": "S02_background.html#make-a-weighted-sampling-map",
    "title": "Background",
    "section": "3.2 Make a weighted sampling map",
    "text": "3.2 Make a weighted sampling map\nThere is a technique we can use to to make a weighted sampling map. Simply counting the number of original observations per cell will indicate where we are most likely to oberve Mola mola.\n\nsamp_weight = rasterize_point_density(obs, mask)\nplot(samp_weight, axes = TRUE, breaks = \"equal\", col = rev(hcl.colors(10)), reset = FALSE)\nplot(coast, col = \"orange\", lwd = 2, add = TRUE)\n\n\n\n\n\n\n\n\nNow let’s take a look at the background, but this time we’ll try to match the count of presences.\n\nconservative_input = sample_background(thinned_obs, samp_weight, \n                              n = 2 * nrow(thinned_obs),\n                              class_label = \"background\",\n                              method = \"bias\",\n                              return_pres = TRUE)\n\nWarning in sample_background(thinned_obs, samp_weight, n = 2 * nrow(thinned_obs), : There are fewer available cells for raster 'NA' (976 presences) than the requested 1952 background points. Only 976 will be returned.\n\ncount(conservative_input, class)\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: -74.15132 ymin: 38.85 xmax: -65.01667 ymax: 45.38333\nGeodetic CRS:  WGS 84\n# A tibble: 2 × 3\n  class          n                                                      geometry\n* &lt;fct&gt;      &lt;int&gt;                                              &lt;MULTIPOINT [°]&gt;\n1 presence     976 ((-65.71667 42.46667), (-65.63333 42.65), (-65.91667 42.9166…\n2 background   976 ((-65.67815 42.4993), (-65.59588 42.66383), (-65.92494 42.91…\n\n\nWhoa - that’s many fewer background points.\n\nplot(conservative_input['class'], \n     axes = TRUE,  \n     pch = \".\", \n     extent = mask, \n     main = \"August conservative class distribution\",\n     reset = FALSE)\nplot(coast, col = \"orange\", add = TRUE)\n\n\n\n\n\n\n\n\nIt appears that background points are essentially shadowing the thinned presence points.",
    "crumbs": [
      "My_background"
    ]
  },
  {
    "objectID": "S02_background.html#a-function-we-can-reuse",
    "href": "S02_background.html#a-function-we-can-reuse",
    "title": "Background",
    "section": "5.1 A function we can reuse",
    "text": "5.1 A function we can reuse\nHere we make a function that needs at least three arguments: the complete set of observations, the mask used for sampling (and possibly thinning) and the month to filter the observations. The pseudo-code might look like this…\nfor a given month\n  filter the obs for that month\n  make the greedy model input by sampling the background\n    save the greedy model input\n  thin the obs\n  make the conservative model input by sampling background\n    save the conservative model input\n  return a list the greedy and conservative model inputs\nPhew! That’s a lot of steps. To manually run those steps 12 times would be tedious, so we roll that into a function that we can reuse 12 times instead.\nThis function will have a name, make_model_input_by_month. It’s a long name, but it makes it obvious what it does. First we start with the documentation.\n\n#' Builds greedy and conservative model input data sets for a given month\n#' \n#' @param mon chr the month abbreviation for the month of interest (\"Jan\" by default)\n#' @param obs table, the complete observation data set\n#' @param raster stars, the object that defines the sampling space, usually a mask\n#' @param species chr, the name of the species prepended to the name of the output files.\n#'   (By default \"Mola mola\" which gets converted to \"Mola_mola\")\n#' @param path the output data path to store this data (be default \"model_input\")\n#' @param min_obs num this sets a threshold below which we wont try to make a model. (Default is 3)\n#' @return a named two element list of greedy and conservative model inputs - they are tables\nmake_model_input_by_month  = function(mon = \"Jan\",\n                                      obs = read_obs(\"Placopecten magellanicus\"),\n                                      raster = NULL,\n                                      species = \"Placopecten magellanicus\",\n                                      path = data_path(\"model_input\"),\n                                      min_obs = 3){\n  # the user *must* provide a raster\n  if (is.null(raster)) stop(\"please provide a raster\")\n  # filter the obs\n  obs = obs |&gt;\n    filter(month == mon[1])\n  \n  # check that we have at least some records, if not enough then alert the user\n  # and return NULL\n  if (nrow(obs) &lt; min_obs){\n    warning(\"sorry, this month has too few records: \", mon)\n    return(NULL)\n  }\n  \n  # make sure the output path exists, if not, make it\n  make_path(path)\n  \n  \n  # make the greedy model input by sampling the background\n  greedy_input = sample_background(obs, raster,\n                                   n = 2 * nrow(obs),\n                                   class_label = \"background\",\n                                   method = c(\"dist_max\", 30000),\n                                   return_pres = TRUE)\n  # save the greedy data\n  filename = sprintf(\"%s-%s-greedy_input.gpkg\", \n                     gsub(\" \", \"_\", species),\n                     mon)\n  write_sf(greedy_input, file.path(path, filename))\n  \n  # thin the obs\n  thinned_obs = thin_by_cell(obs, raster)\n  \n  # sampling weight\n  samp_weight = rasterize_point_density(obs, raster)\n  \n  # make the conservative model\n  conservative_input = sample_background(thinned_obs, samp_weight,\n                                   n = 2 * nrow(thinned_obs),\n                                   class_label = \"background\",\n                                   method = \"bias\",\n                                   return_pres = TRUE)\n  \n  # save the conservative data\n  filename = sprintf(\"%s-%s-conservative_input.gpkg\", \n                     gsub(\" \", \"_\", species),\n                     mon)\n  write_sf(conservative_input, file.path(path,filename))\n  \n  # make a list\n  r = list(greedy = greedy_input, conservative = conservative_input)\n  \n  # return, but disable automatic printing\n  invisible(r)\n}",
    "crumbs": [
      "My_background"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Brought to you by the Tandy Center for Ocean Forecasting at Bigelow Laboratory for Ocean Science and Colby College.\n\n1 Contacts\nDr. Nick Record\nBen Tupper\nRaising questions or issues: If you have a question, start a new “issue” on the github issues tab. If a question has been posed by another, and you think you can help with the answer then please feel free to respond.\n\n\n2 Website\nWe build the website using quarto which is perfect from transforming [RMarkdown](https://rmarkdown.rstudio.com/ pages into a website with minimal investment. See this wiki page if you would like to add your work to you own fork of the class repository.\n\n\n\n\n Back to top",
    "crumbs": [
      "About"
    ]
  }
]